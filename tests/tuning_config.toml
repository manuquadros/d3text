optimizer = ["adamw", "adam", "nadam"]
lr = [0.001, 0.01, 0.002, 0.0005]
lr_scheduler = ["reduce_on_plateau", "exponential"]
dropout = [0.1, 0.2, 0.4]
hidden_layers = [4096, 2048, 1024, 512]
normalization = ["layer", "batch"]
batch_size = [12]
num_epochs = [50]
patience = [2]
base_model = ["michiyasunaga/BioLinkBERT-base", "prajjwal1/bert-mini"]
base_layers_to_unfreeze = [0, 1, 2]
model_class = ["ETEBrendaModel"]
