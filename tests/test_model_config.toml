optimizer = "adamw"
lr = 0.0001
lr_scheduler = "reduce_on_plateau"
dropout = 0.2
hidden_layers = [1024, 1024, 2048]
normalization = "layer"
batch_size = 8
num_epochs = 1
patience = 1
base_model = "prajjwal1/bert-mini"
#base_model = "michiyasunaga/BioLinkBERT-base"
base_layers_to_unfreeze = 2
#model_class = "ETEBrendaClassifier"
model_class = "ETEBrendaModel"
embedding_size = 256
